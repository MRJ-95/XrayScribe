{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAmYZMbravKM",
        "outputId": "1580d5fa-d7c5-41b9-8dcf-258d11871878"
      },
      "id": "dAmYZMbravKM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount the root Google Drive directory first\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Then, you can access your specific folder within the Drive\n",
        "import os\n",
        "# Create the target directory if it doesn't exist\n",
        "target_dir = '/content/drive/MyDrive/IU-Xray' # Assuming your Drive path is 'MyDrive/IU-Xray'\n",
        "if not os.path.exists(target_dir):\n",
        "  os.makedirs(target_dir)\n",
        "\n",
        "print(f\"IU-Xray directory is mounted at: {target_dir}\")"
      ],
      "metadata": {
        "id": "hXuzwyMqZ3vB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e8454b8-0427-49ea-d7a7-a97a53de07e0"
      },
      "id": "hXuzwyMqZ3vB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "IU-Xray directory is mounted at: /content/drive/MyDrive/IU-Xray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLQjDUcZDqAF",
        "outputId": "767ee1b7-6604-416e-d004-85eee337b30d"
      },
      "id": "iLQjDUcZDqAF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Radiology Report Generator with Medical Context\n",
        "# This implementation incorporates clinical data to improve accuracy\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import random\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
      ],
      "metadata": {
        "id": "6ShlBg-FULiK"
      },
      "id": "6ShlBg-FULiK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration parameters\n",
        "CONFIG = {\n",
        "    'image_size': 128,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 5,\n",
        "    'learning_rate': 2e-5,\n",
        "    'max_report_length': 256,\n",
        "    't5_model_name': 'google/flan-t5-small',  # More powerful model for medical context\n",
        "    'image_encoder': 'resnet18',\n",
        "    'hidden_dim': 768,\n",
        "    'seed': 42,\n",
        "    # Clinical finding categories (based on CheXpert labels)\n",
        "    'clinical_findings': [\n",
        "        'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly',\n",
        "        'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation',\n",
        "        'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion',\n",
        "        'Pleural Other', 'Fracture', 'Support Devices'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(CONFIG['seed'])\n",
        "np.random.seed(CONFIG['seed'])"
      ],
      "metadata": {
        "id": "GookiBy7UUi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c487b8-8656-4c49-8886-ee53d4a8243c"
      },
      "id": "GookiBy7UUi-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n",
        "# 1. Data Processing with Medical Context\n",
        "###########################################"
      ],
      "metadata": {
        "id": "MQGJ8wFJUXRw"
      },
      "id": "MQGJ8wFJUXRw"
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedChestXRayDataset(Dataset):\n",
        "    def __init__(self, image_paths, reports, clinical_data=None, tokenizer=None, transform=None, max_length=512):\n",
        "        \"\"\"\n",
        "        Enhanced dataset that includes clinical context data\n",
        "\n",
        "        Args:\n",
        "            image_paths: List of paths to the images\n",
        "            reports: List of corresponding reports\n",
        "            clinical_data: DataFrame with clinical context (patient history, findings, etc.)\n",
        "            tokenizer: Tokenizer for text processing\n",
        "            transform: Image transformations\n",
        "            max_length: Maximum token length for reports\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.reports = reports\n",
        "        self.clinical_data = clinical_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load and transform image\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get clinical context if available\n",
        "        clinical_context = \"\"\n",
        "        if self.clinical_data is not None:\n",
        "            # Extract study ID from image path\n",
        "            study_id = os.path.basename(image_path).split('_')[0]\n",
        "\n",
        "            # Get clinical data for this study\n",
        "            patient_data = self.clinical_data.get(study_id, {})\n",
        "\n",
        "            # Format clinical context\n",
        "            if patient_data:\n",
        "                age = patient_data.get('age', 'Unknown')\n",
        "                sex = patient_data.get('sex', 'Unknown')\n",
        "                history = patient_data.get('history', 'No history provided')\n",
        "\n",
        "                clinical_context = f\"Patient: {age} year old {sex}. History: {history}. \"\n",
        "\n",
        "        # Tokenize report with clinical context as prefix\n",
        "        report = self.reports[idx]\n",
        "\n",
        "        # Create prompt with clinical context\n",
        "        prompt = f\"generate radiology report: {clinical_context}\"\n",
        "\n",
        "        tokenized_input = self.tokenizer(\n",
        "            prompt,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Prepare decoder input ids (needed for T5)\n",
        "        decoder_input_ids = self.tokenizer(\n",
        "            \"\",\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        # Get labels (target)\n",
        "        labels = self.tokenizer(\n",
        "            report,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'input_ids': tokenized_input.input_ids.squeeze(),\n",
        "            'attention_mask': tokenized_input.attention_mask.squeeze(),\n",
        "            'decoder_input_ids': decoder_input_ids.squeeze(),\n",
        "            'labels': labels.squeeze(),\n",
        "            'report': report,\n",
        "            'clinical_context': clinical_context,\n",
        "            'image_path': image_path\n",
        "        }\n",
        "\n",
        "def load_indiana_cxr_data(base_dir):\n",
        "    \"\"\"\n",
        "    Updated data loader that handles .dcm.png filenames correctly\n",
        "    \"\"\"\n",
        "    # Path configuration\n",
        "    img_dir = os.path.join(base_dir, 'images', 'images_normalized')\n",
        "    reports_csv = os.path.join(base_dir, 'indiana_reports.csv')\n",
        "    projections_csv = os.path.join(base_dir, 'indiana_projections.csv')\n",
        "\n",
        "    # Load data\n",
        "    reports_df = pd.read_csv(reports_csv)\n",
        "    projections_df = pd.read_csv(projections_csv)\n",
        "\n",
        "    # Merge data while keeping original filenames\n",
        "    merged_df = pd.merge(\n",
        "        projections_df,\n",
        "        reports_df,\n",
        "        on='uid',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # Process images and reports\n",
        "    image_paths = []\n",
        "    reports_processed = []\n",
        "    clinical_data = {}\n",
        "    patient_studies = {}\n",
        "\n",
        "    for idx, row in merged_df.iterrows():\n",
        "        # Construct image path with .dcm.png extension\n",
        "        img_file = row['filename']\n",
        "        image_path = os.path.join(img_dir, img_file)\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Warning: Missing image {img_file} - skipping entry\")\n",
        "            continue\n",
        "\n",
        "        # Extract patient ID from filename (e.g., '9' from '9_IM-2407-1001.dcm.png')\n",
        "        patient_id = img_file.split('_')[0]\n",
        "\n",
        "        # Create report\n",
        "        findings = row['findings'] if pd.notna(row['findings']) else \"No findings reported\"\n",
        "        impression = row['impression'] if pd.notna(row['impression']) else \"No impression provided\"\n",
        "        final_report = (\n",
        "            f\"Projection: {row['projection']}\\n\"\n",
        "            f\"Findings: {findings}\\n\"\n",
        "            f\"Impression: {impression}\"\n",
        "        )\n",
        "\n",
        "        # Store data\n",
        "        image_paths.append(image_path)\n",
        "        reports_processed.append(final_report)\n",
        "\n",
        "        # Track patient studies for splitting\n",
        "        study_id = os.path.splitext(img_file)[0]  # '9_IM-2407-1001.dcm'\n",
        "        clinical_data[study_id] = {\n",
        "            'patient_id': patient_id,\n",
        "            'age': str(row.get('patient_age', 'Unknown')),\n",
        "            'sex': row.get('patient_gender', 'Unknown'),\n",
        "            'history': row.get('indication', 'No history provided')\n",
        "        }\n",
        "        patient_studies[study_id] = patient_id\n",
        "\n",
        "    # Create patient-based splits\n",
        "    unique_patients = list(set(patient_studies.values()))\n",
        "\n",
        "    # Ensure we have enough patients for splitting\n",
        "    if len(unique_patients) < 2:\n",
        "        raise ValueError(f\"Only {len(unique_patients)} patients found - need at least 2 for splitting\")\n",
        "\n",
        "    train_patients, temp_patients = train_test_split(\n",
        "        unique_patients,\n",
        "        test_size=0.3,\n",
        "        random_state=CONFIG['seed']\n",
        "    )\n",
        "    val_patients, test_patients = train_test_split(\n",
        "        temp_patients,\n",
        "        test_size=0.5,\n",
        "        random_state=CONFIG['seed']\n",
        "    )\n",
        "\n",
        "    # Create splits\n",
        "    def get_split(patient_ids):\n",
        "        return [\n",
        "            (path, report)\n",
        "            for path, report, study_id in zip(image_paths, reports_processed, patient_studies.keys())\n",
        "            if patient_studies[study_id] in patient_ids\n",
        "        ]\n",
        "\n",
        "    train_data = get_split(train_patients)\n",
        "    val_data = get_split(val_patients)\n",
        "    test_data = get_split(test_patients)\n",
        "\n",
        "    # Unpack the data\n",
        "    def unpack(data):\n",
        "        return list(zip(*data)) if data else ([], [])\n",
        "\n",
        "    train_img, train_rep = unpack(train_data)\n",
        "    val_img, val_rep = unpack(val_data)\n",
        "    test_img, test_rep = unpack(test_data)\n",
        "\n",
        "    return (\n",
        "        train_img, val_img, test_img,\n",
        "        train_rep, val_rep, test_rep,\n",
        "        clinical_data\n",
        "    )"
      ],
      "metadata": {
        "id": "AXcQgr5AUmJA"
      },
      "id": "AXcQgr5AUmJA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n",
        "# 2. Enhanced Model Architecture\n",
        "###########################################"
      ],
      "metadata": {
        "id": "1RI_nGhQUm3x"
      },
      "id": "1RI_nGhQUm3x"
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedRadiologyReportGenerator(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        \"\"\"\n",
        "        Enhanced model that incorporates clinical context and uses a multi-task approach\n",
        "        for more accurate report generation\n",
        "\n",
        "        Args:\n",
        "            config: Configuration dictionary with model parameters\n",
        "        \"\"\"\n",
        "        super(EnhancedRadiologyReportGenerator, self).__init__()\n",
        "\n",
        "        # Image encoder (same as before)\n",
        "        if config['image_encoder'] == 'resnet50':\n",
        "            self.image_encoder = models.resnet50(weights='IMAGENET1K_V2')\n",
        "            self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-1])\n",
        "            self.image_features_dim = 2048\n",
        "\n",
        "        elif config['image_encoder'] == 'densenet121':\n",
        "            self.image_encoder = models.densenet121(weights='IMAGENET1K_V1')\n",
        "            self.image_encoder.classifier = nn.Identity()\n",
        "            self.image_features_dim = 1024\n",
        "\n",
        "        elif config['image_encoder'] == 'vit':\n",
        "            self.image_encoder = models.vit_b_16(weights='IMAGENET1K_V1')\n",
        "            self.image_encoder.heads = nn.Identity()\n",
        "            self.image_features_dim = 768\n",
        "        elif config['image_encoder'] == 'resnet18':\n",
        "            self.image_encoder = models.resnet18(weights='IMAGENET1K_V1')\n",
        "            # Do not convert to sequential, keep the original structure\n",
        "            # self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-1])\n",
        "            # self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-2]) # Remove the avgpool and fc layers\n",
        "            self.image_encoder = nn.Sequential(*list(self.image_encoder.children())[:-1]) # Keep the avgpool layer\n",
        "            self.image_features_dim = 512\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown image encoder: {config['image_encoder']}\")\n",
        "\n",
        "\n",
        "        # Freeze image encoder layers\n",
        "        for param in self.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Optional: Unfreeze last layer\n",
        "        # Access layer4 directly since it's now part of the model\n",
        "        # for param in self.image_encoder[-1].parameters(): # Now accessing the last block, which should be layer4\n",
        "        #     param.requires_grad = True\n",
        "        for param in self.image_encoder[-1].parameters(): # Now accessing AdaptiveAvgPool2d\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Image feature projection\n",
        "        self.image_projection = nn.Sequential(\n",
        "            nn.Linear(self.image_features_dim, config['hidden_dim']),\n",
        "            nn.LayerNorm(config['hidden_dim']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(config['hidden_dim'], config['hidden_dim'])\n",
        "        )\n",
        "\n",
        "        # T5 text generator\n",
        "        self.text_decoder = T5ForConditionalGeneration.from_pretrained(config['t5_model_name'])\n",
        "\n",
        "        # Additional multi-task components for medical accuracy\n",
        "        self.finding_classifier = nn.Linear(config['hidden_dim'], len(config['clinical_findings']))\n",
        "\n",
        "        # Cross-attention mechanism (enhanced)\n",
        "        t5_config = self.text_decoder.config\n",
        "        self.image_to_t5_projection = nn.Linear(\n",
        "            config['hidden_dim'],\n",
        "            t5_config.d_model\n",
        "        )\n",
        "\n",
        "        # Clinical context attention\n",
        "        self.clinical_context_attention = nn.MultiheadAttention(\n",
        "            embed_dim=config['hidden_dim'],\n",
        "            num_heads=8,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Store config\n",
        "        self.config = config\n",
        "\n",
        "    def forward(self, images, input_ids=None, attention_mask=None, decoder_input_ids=None, labels=None, finding_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass with enhanced medical context integration\n",
        "\n",
        "        Args:\n",
        "            images: Batch of X-ray images\n",
        "            input_ids: Input token IDs (including clinical context)\n",
        "            attention_mask: Attention mask for input\n",
        "            decoder_input_ids: Input IDs for the decoder\n",
        "            labels: Target token IDs\n",
        "            finding_labels: Labels for clinical findings (for multi-task learning)\n",
        "        \"\"\"\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        # Extract image features\n",
        "        image_features = self.image_encoder(images)\n",
        "        if len(image_features.shape) == 4:  # For CNNs\n",
        "            image_features = image_features.squeeze(-1).squeeze(-1)\n",
        "\n",
        "        # Project image features\n",
        "        image_features = self.image_projection(image_features)\n",
        "\n",
        "        # Multi-task prediction of findings (improves feature extraction)\n",
        "        finding_preds = None\n",
        "        if self.training and finding_labels is not None:\n",
        "            finding_preds = self.finding_classifier(image_features)\n",
        "\n",
        "        # Process text inputs (including clinical context)\n",
        "        if self.training:\n",
        "            # During training we use the full encoder-decoder with teacher forcing\n",
        "\n",
        "            # Get T5 encoder outputs first\n",
        "            encoder_outputs = self.text_decoder.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_dict=True\n",
        "            )\n",
        "            encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "            # Project image features to T5 dimension\n",
        "            image_features_t5 = self.image_to_t5_projection(image_features)\n",
        "\n",
        "            # Add image features to encoder outputs (medical context integration)\n",
        "            image_features_expanded = image_features_t5.unsqueeze(1).expand(-1, encoder_hidden_states.size(1), -1)\n",
        "\n",
        "            # Enhanced context processing - attention-based fusion\n",
        "            enhanced_encoder_states = encoder_hidden_states + image_features_expanded\n",
        "\n",
        "            # Forward through decoder\n",
        "            outputs = self.text_decoder(\n",
        "                encoder_outputs=(enhanced_encoder_states,),\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                labels=labels,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            # Return with finding predictions if doing multi-task learning\n",
        "            if finding_preds is not None:\n",
        "                return outputs.loss, finding_preds\n",
        "            else:\n",
        "                return outputs\n",
        "\n",
        "        else:\n",
        "            # During inference we generate text\n",
        "            # First encode the input prompt\n",
        "            encoder_outputs = self.text_decoder.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                return_dict=True\n",
        "            )\n",
        "\n",
        "            # Add image features to encoder outputs\n",
        "            encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "            # Project image features to T5 dimension\n",
        "            image_features_t5 = self.image_to_t5_projection(image_features)\n",
        "\n",
        "            # Add image features to encoder outputs\n",
        "            image_features_expanded = image_features_t5.unsqueeze(1).expand(-1, encoder_hidden_states.size(1), -1)\n",
        "            enhanced_encoder_states = encoder_hidden_states + image_features_expanded\n",
        "\n",
        "            # Generate report using the enhanced encoder states\n",
        "            generated_ids = self.text_decoder.generate(\n",
        "                encoder_outputs=(enhanced_encoder_states,),\n",
        "                max_length=self.config['max_report_length'],\n",
        "                num_beams=5,  # More beams for better quality\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True,\n",
        "                repetition_penalty=1.2,  # Avoid repetitive text\n",
        "                use_cache=True,\n",
        "                do_sample=True,  # Enable sampling for more diverse outputs\n",
        "                top_p=0.9,  # Nucleus sampling\n",
        "                temperature=0.7  # Temperature for sampling\n",
        "            )\n",
        "\n",
        "            return generated_ids"
      ],
      "metadata": {
        "id": "IU5V_nkHUpya"
      },
      "id": "IU5V_nkHUpya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n",
        "# 3. Enhanced Training & Evaluation\n",
        "###########################################"
      ],
      "metadata": {
        "id": "O0BhGLFSUtE7"
      },
      "id": "O0BhGLFSUtE7"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler  # ADD THIS AT TOP\n",
        "\n",
        "def train_epoch_enhanced(model, dataloader, optimizer, scheduler, device, clinical_findings=None):\n",
        "    \"\"\"Training with multi-task learning + mixed precision\"\"\"\n",
        "    model.train()\n",
        "    scaler = GradScaler()  # INITIALIZE SCALER HERE\n",
        "    epoch_loss = 0\n",
        "    report_loss = 0\n",
        "    finding_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move batch to device\n",
        "        images = batch['image'].to(device, non_blocking=True)  # ADD NON_BLOCKING\n",
        "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "        decoder_input_ids = batch['decoder_input_ids'].to(device, non_blocking=True)\n",
        "        labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "        # Initialize finding labels\n",
        "        finding_labels = None\n",
        "        if clinical_findings and 'clinical_labels' in batch:\n",
        "            finding_labels = batch['clinical_labels'].to(device, non_blocking=True)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with mixed precision -------------------------\n",
        "        with autocast():  # WRAP FORWARD PASS IN AUTOCAST\n",
        "            if finding_labels is not None:\n",
        "                loss_reports, finding_preds = model(\n",
        "                    images=images,\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_input_ids=decoder_input_ids,\n",
        "                    labels=labels,\n",
        "                    finding_labels=finding_labels\n",
        "                )\n",
        "                finding_loss_val = F.binary_cross_entropy_with_logits(finding_preds, finding_labels)\n",
        "                loss = loss_reports + 0.2 * finding_loss_val\n",
        "            else:\n",
        "                outputs = model(\n",
        "                    images=images,\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_input_ids=decoder_input_ids,\n",
        "                    labels=labels\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "        # -----------------------------------------------------------\n",
        "\n",
        "        # Backward pass with scaler --------------------------------\n",
        "        scaler.scale(loss).backward()  # REPLACE loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        scaler.unscale_(optimizer)  # NEED TO UNSCALE BEFORE CLIPPING\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        scaler.step(optimizer)  # REPLACE optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({\n",
        "            \"loss\": loss.item(),\n",
        "            \"report_loss\": report_loss / (progress_bar.n + 1),\n",
        "            \"finding_loss\": finding_loss / (progress_bar.n + 1) if finding_labels is not None else 0\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'total_loss': epoch_loss / len(dataloader),\n",
        "        'report_loss': report_loss / len(dataloader),\n",
        "        'finding_loss': finding_loss / len(dataloader) if clinical_findings else 0\n",
        "    }\n",
        "\n",
        "def evaluate_enhanced(model, dataloader, tokenizer, device, clinical_findings=None):\n",
        "    \"\"\"Enhanced evaluation with clinical accuracy metrics\"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    generated_reports = []\n",
        "    reference_reports = []\n",
        "    clinical_contexts = []\n",
        "\n",
        "    # For clinical accuracy\n",
        "    if clinical_findings:\n",
        "        finding_preds = []\n",
        "        finding_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "        for batch in progress_bar:\n",
        "            # Move batch to device\n",
        "            images = batch['image'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # For clinical accuracy\n",
        "            if clinical_findings and 'clinical_labels' in batch:\n",
        "                batch_finding_labels = batch['clinical_labels'].to(device)\n",
        "                finding_labels.append(batch_finding_labels.cpu().numpy())\n",
        "\n",
        "            # For loss calculation\n",
        "            outputs = model(\n",
        "                images=images,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate reports\n",
        "            generated_ids = model(\n",
        "                images=images,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            # Decode the generated and reference reports\n",
        "            gen_reports = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            generated_reports.extend(gen_reports)\n",
        "            reference_reports.extend(batch['report'])\n",
        "\n",
        "            if 'clinical_context' in batch:\n",
        "                clinical_contexts.extend(batch['clinical_context'])\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    references = [[report.split()] for report in reference_reports]\n",
        "    candidates = [report.split() for report in generated_reports]\n",
        "    bleu_score = corpus_bleu(references, candidates)\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {metric: 0.0 for metric in ['rouge1', 'rouge2', 'rougeL']}\n",
        "\n",
        "    for gen, ref in zip(generated_reports, reference_reports):\n",
        "        scores = scorer.score(ref, gen)\n",
        "        for metric in rouge_scores:\n",
        "            rouge_scores[metric] += scores[metric].fmeasure\n",
        "\n",
        "    # Average ROUGE scores\n",
        "    for metric in rouge_scores:\n",
        "        rouge_scores[metric] /= len(generated_reports)\n",
        "\n",
        "    # Clinical accuracy metrics\n",
        "    clinical_metrics = {}\n",
        "    if clinical_findings and finding_labels:\n",
        "        finding_labels = np.concatenate(finding_labels, axis=0)\n",
        "        finding_preds = np.concatenate(finding_preds, axis=0)\n",
        "        finding_preds_binary = (finding_preds > 0.5).astype(int)\n",
        "\n",
        "        # Calculate precision, recall, F1\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            finding_labels, finding_preds_binary, average='macro'\n",
        "        )\n",
        "\n",
        "        accuracy = accuracy_score(finding_labels, finding_preds_binary)\n",
        "\n",
        "        clinical_metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    # Sample predictions\n",
        "    samples = []\n",
        "    for i in range(min(5, len(generated_reports))):\n",
        "        context = clinical_contexts[i] if clinical_contexts else \"\"\n",
        "        samples.append({\n",
        "            'clinical_context': context,\n",
        "            'generated': generated_reports[i],\n",
        "            'reference': reference_reports[i]\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        'loss': val_loss / len(dataloader),\n",
        "        'bleu': bleu_score,\n",
        "        'rouge': rouge_scores,\n",
        "        'clinical_metrics': clinical_metrics,\n",
        "        'samples': samples\n",
        "    }\n",
        "\n",
        "def analyze_medical_accuracy(generated_reports, reference_reports, medical_terms=None):\n",
        "    \"\"\"\n",
        "    Analyze the medical accuracy of generated reports using medical terminology\n",
        "\n",
        "    Args:\n",
        "        generated_reports: List of generated reports\n",
        "        reference_reports: List of reference reports\n",
        "        medical_terms: Dictionary of important medical terms to check\n",
        "    \"\"\"\n",
        "    if medical_terms is None:\n",
        "        # Common radiology findings and terms\n",
        "        medical_terms = {\n",
        "            'cardiomegaly': ['cardiomegaly', 'enlarged heart', 'cardiac enlargement'],\n",
        "            'edema': ['edema', 'pulmonary edema', 'fluid overload'],\n",
        "            'pneumonia': ['pneumonia', 'consolidation', 'infectious process'],\n",
        "            'pleural_effusion': ['pleural effusion', 'effusion', 'fluid in pleural space'],\n",
        "            'pneumothorax': ['pneumothorax', 'collapsed lung'],\n",
        "            'atelectasis': ['atelectasis', 'lung collapse', 'loss of lung volume'],\n",
        "            'nodule': ['nodule', 'mass', 'opacity'],\n",
        "            'normal': ['normal', 'no acute findings', 'no acute abnormality']\n",
        "        }\n",
        "\n",
        "    # Initialize results\n",
        "    term_precision = {term: 0 for term in medical_terms}\n",
        "    term_recall = {term: 0 for term in medical_terms}\n",
        "    term_counts_ref = {term: 0 for term in medical_terms}\n",
        "    term_counts_gen = {term: 0 for term in medical_terms}\n",
        "\n",
        "    for gen_report, ref_report in zip(generated_reports, reference_reports):\n",
        "        gen_report = gen_report.lower()\n",
        "        ref_report = ref_report.lower()\n",
        "\n",
        "        for term, synonyms in medical_terms.items():\n",
        "            # Check if term in reference report\n",
        "            term_in_ref = any(synonym in ref_report for synonym in synonyms)\n",
        "            term_in_gen = any(synonym in gen_report for synonym in synonyms)\n",
        "\n",
        "            if term_in_ref:\n",
        "                term_counts_ref[term] += 1\n",
        "                if term_in_gen:\n",
        "                    term_recall[term] += 1\n",
        "\n",
        "            if term_in_gen:\n",
        "                term_counts_gen[term] += 1\n",
        "                if term_in_ref:\n",
        "                    term_precision[term] += 1\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision_results = {}\n",
        "    recall_results = {}\n",
        "    f1_results = {}\n",
        "\n",
        "    for term in medical_terms:\n",
        "        if term_counts_gen[term] > 0:\n",
        "            precision_results[term] = term_precision[term] / term_counts_gen[term]\n",
        "        else:\n",
        "            precision_results[term] = 0\n",
        "\n",
        "        if term_counts_ref[term] > 0:\n",
        "            recall_results[term] = term_recall[term] / term_counts_ref[term]\n",
        "        else:\n",
        "            recall_results[term] = 0\n",
        "\n",
        "        # F1 score\n",
        "        if precision_results[term] + recall_results[term] > 0:\n",
        "            f1_results[term] = 2 * precision_results[term] * recall_results[term] / (precision_results[term] + recall_results[term])\n",
        "        else:\n",
        "            f1_results[term] = 0\n",
        "\n",
        "    # Calculate macro averages\n",
        "    avg_precision = sum(precision_results.values()) / len(precision_results)\n",
        "    avg_recall = sum(recall_results.values()) / len(recall_results)\n",
        "    avg_f1 = sum(f1_results.values()) / len(f1_results)\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    correct_negations = analyze_negations(generated_reports, reference_reports)\n",
        "\n",
        "    return {\n",
        "        'precision': precision_results,\n",
        "        'recall': recall_results,\n",
        "        'f1': f1_results,\n",
        "        'avg_precision': avg_precision,\n",
        "        'avg_recall': avg_recall,\n",
        "        'avg_f1': avg_f1,\n",
        "        'negation_accuracy': correct_negations\n",
        "    }\n",
        "\n",
        "def analyze_negations(generated_reports, reference_reports):\n",
        "    \"\"\"\n",
        "    Analyze how well the model handles negations in medical reports\n",
        "\n",
        "    Args:\n",
        "        generated_reports: List of generated reports\n",
        "        reference_reports: List of reference reports\n",
        "    \"\"\"\n",
        "    # Common negation phrases\n",
        "    negation_phrases = ['no', 'not', 'without', 'free of', 'absence of', 'negative for', 'clear of', 'ruled out']\n",
        "\n",
        "    # Common medical conditions that are frequently negated\n",
        "    conditions = [\n",
        "        'pneumothorax', 'effusion', 'infiltrate', 'consolidation', 'edema',\n",
        "        'cardiomegaly', 'pneumonia', 'nodule', 'fracture', 'mass'\n",
        "    ]\n",
        "\n",
        "    negation_accuracy = {}\n",
        "    total_negations = 0\n",
        "    correct_negations = 0\n",
        "\n",
        "    for condition in conditions:\n",
        "        negation_accuracy[condition] = {\n",
        "            'total': 0,\n",
        "            'correct': 0\n",
        "        }\n",
        "\n",
        "    for gen_report, ref_report in zip(generated_reports, reference_reports):\n",
        "        gen_lower = gen_report.lower()\n",
        "        ref_lower = ref_report.lower()\n",
        "\n",
        "        for condition in conditions:\n",
        "            # Check for negated conditions in reference report\n",
        "            for neg in negation_phrases:\n",
        "                negation_pattern = f\"{neg} {condition}\"\n",
        "\n",
        "                # If negation exists in reference\n",
        "                if negation_pattern in ref_lower:\n",
        "                    negation_accuracy[condition]['total'] += 1\n",
        "                    total_negations += 1\n",
        "\n",
        "                    # Check if correctly negated in generated report\n",
        "                    if any(f\"{neg} {condition}\" in gen_lower for neg in negation_phrases):\n",
        "                        negation_accuracy[condition]['correct'] += 1\n",
        "                        correct_negations += 1\n",
        "                    # Check if incorrectly affirmed\n",
        "                    elif condition in gen_lower and not any(f\"{neg} {condition}\" in gen_lower for neg in negation_phrases):\n",
        "                        pass  # Incorrectly affirmed - don't increment correct\n",
        "\n",
        "    # Calculate accuracy\n",
        "    for condition in conditions:\n",
        "        if negation_accuracy[condition]['total'] > 0:\n",
        "            negation_accuracy[condition]['accuracy'] = negation_accuracy[condition]['correct'] / negation_accuracy[condition]['total']\n",
        "        else:\n",
        "            negation_accuracy[condition]['accuracy'] = 0\n",
        "\n",
        "    overall_accuracy = correct_negations / total_negations if total_negations > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'condition_accuracy': negation_accuracy,\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'total_negations': total_negations,\n",
        "        'correct_negations': correct_negations\n",
        "    }"
      ],
      "metadata": {
        "id": "XP9yZ6PJUu1G"
      },
      "id": "XP9yZ6PJUu1G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n",
        "# 4. Custom Clinical Accuracy Metrics\n",
        "###########################################"
      ],
      "metadata": {
        "id": "E5WZ_B7LUzzk"
      },
      "id": "E5WZ_B7LUzzk"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chexbert_score(generated_reports, reference_reports, chexbert_model_path=None):\n",
        "    \"\"\"\n",
        "    Calculate CheXbert score for radiology report evaluation\n",
        "    This is a specialized metric for measuring clinical accuracy of radiology reports\n",
        "\n",
        "    CheXbert requires a specialized model which must be downloaded separately\n",
        "    See: https://github.com/stanfordmlgroup/CheXbert\n",
        "\n",
        "    Args:\n",
        "        generated_reports: List of generated reports\n",
        "        reference_reports: List of reference reports\n",
        "        chexbert_model_path: Path to CheXbert model\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        from chexbert import CheXbertLabeler\n",
        "\n",
        "        if chexbert_model_path is None:\n",
        "            print(\"CheXbert model path not provided, skipping CheXbert evaluation\")\n",
        "            return None\n",
        "\n",
        "        # Initialize CheXbert labeler\n",
        "        labeler = CheXbertLabeler(chexbert_model_path, device=device)\n",
        "\n",
        "        # Get labels for generated and reference reports\n",
        "        gen_labels = labeler.label(generated_reports)\n",
        "        ref_labels = labeler.label(reference_reports)\n",
        "\n",
        "        # Calculate accuracy, precision, recall, F1 score for each label\n",
        "        label_metrics = {}\n",
        "        conditions = labeler.get_conditions()\n",
        "\n",
        "        for i, condition in enumerate(conditions):\n",
        "            true_pos = 0\n",
        "            false_pos = 0\n",
        "            false_neg = 0\n",
        "            true_neg = -0\n",
        "\n",
        "            for gen_label, ref_label in zip(gen_labels, ref_labels):\n",
        "                # CheXbert uses 0 for negative, 1 for positive\n",
        "                gen_positive = gen_label[i] == 1\n",
        "                ref_positive = ref_label[i] == 1\n",
        "\n",
        "                if gen_positive and ref_positive:\n",
        "                    true_pos += 1\n",
        "                elif gen_positive and not ref_positive:\n",
        "                    false_pos += 1\n",
        "                elif not gen_positive and ref_positive:\n",
        "                    false_neg += 1\n",
        "                else:\n",
        "                    true_neg += 1\n",
        "\n",
        "            # Calculate metrics\n",
        "            precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
        "            recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
        "\n",
        "            label_metrics[condition] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'accuracy': accuracy\n",
        "            }\n",
        "\n",
        "        # Calculate overall metrics\n",
        "        overall_metrics = {\n",
        "            'precision': sum(m['precision'] for m in label_metrics.values()) / len(label_metrics),\n",
        "            'recall': sum(m['recall'] for m in label_metrics.values()) / len(label_metrics),\n",
        "            'f1': sum(m['f1'] for m in label_metrics.values()) / len(label_metrics),\n",
        "            'accuracy': sum(m['accuracy'] for m in label_metrics.values()) / len(label_metrics)\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'label_metrics': label_metrics,\n",
        "            'overall': overall_metrics\n",
        "        }\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"CheXbert package not installed, skipping CheXbert evaluation\")\n",
        "        return None\n",
        "\n",
        "def calculate_radgraph_score(generated_reports, reference_reports, radgraph_path=None):\n",
        "    \"\"\"\n",
        "    Calculate RadGraph score for radiology report evaluation\n",
        "    RadGraph is a specialized graph-based metric for radiology report evaluation\n",
        "\n",
        "    See: https://github.com/ncbi-nlp/RadGraph\n",
        "\n",
        "    Args:\n",
        "        generated_reports: List of generated reports\n",
        "        reference_reports: List of reference reports\n",
        "        radgraph_path: Path to RadGraph model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import spacy\n",
        "        from radgraph import RadGraph\n",
        "\n",
        "        if radgraph_path is None:\n",
        "            print(\"RadGraph model path not provided, skipping RadGraph evaluation\")\n",
        "            return None\n",
        "\n",
        "        # Initialize RadGraph\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        radgraph = RadGraph(nlp)\n",
        "\n",
        "        total_precision = 0\n",
        "        total_recall = 0\n",
        "        total_f1 = 0\n",
        "\n",
        "        for gen_report, ref_report in zip(generated_reports, reference_reports):\n",
        "            # Extract entities and relations from reports\n",
        "            gen_graph = radgraph(gen_report)\n",
        "            ref_graph = radgraph(ref_report)\n",
        "\n",
        "            # Get entities and relationships\n",
        "            gen_entities = set([(ent.text, ent.label_) for ent in gen_graph.ents])\n",
        "            ref_entities = set([(ent.text, ent.label_) for ent in ref_graph.ents])\n",
        "\n",
        "            # Get relations\n",
        "            gen_relations = set([(rel[0].text, rel[1], rel[2].text) for rel in gen_graph.relations])\n",
        "            ref_relations = set([(rel[0].text, rel[1], rel[2].text) for rel in ref_graph.relations])\n",
        "\n",
        "            # Calculate entity metrics\n",
        "            common_entities = gen_entities.intersection(ref_entities)\n",
        "            precision_entities = len(common_entities) / len(gen_entities) if len(gen_entities) > 0 else 0\n",
        "            recall_entities = len(common_entities) / len(ref_entities) if len(ref_entities) > 0 else 0\n",
        "\n",
        "            # Calculate relation metrics\n",
        "            common_relations = gen_relations.intersection(ref_relations)\n",
        "            precision_relations = len(common_relations) / len(gen_relations) if len(gen_relations) > 0 else 0\n",
        "            recall_relations = len(common_relations) / len(ref_relations) if len(ref_relations) > 0 else 0\n",
        "\n",
        "            # Combined metrics\n",
        "            precision = (precision_entities + precision_relations) / 2\n",
        "            recall = (recall_entities + recall_relations) / 2\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            total_precision += precision\n",
        "            total_recall += recall\n",
        "            total_f1 += f1\n",
        "\n",
        "        # Average metrics\n",
        "        avg_precision = total_precision / len(generated_reports)\n",
        "        avg_recall = total_recall / len(generated_reports)\n",
        "        avg_f1 = total_f1 / len(generated_reports)\n",
        "\n",
        "        return {\n",
        "            'precision': avg_precision,\n",
        "            'recall': avg_recall,\n",
        "            'f1': avg_f1\n",
        "        }\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"RadGraph package not installed, skipping RadGraph evaluation\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "mCcA2hb0U2L9"
      },
      "id": "mCcA2hb0U2L9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n",
        "# 5. Main Training Loop with Medical Context\n",
        "###########################################"
      ],
      "metadata": {
        "id": "RoYRivR5U633"
      },
      "id": "RoYRivR5U633"
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to train and evaluate the enhanced model\"\"\"\n",
        "\n",
        "    # Set up image transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "    print(\"Loading IU X-Ray dataset from Google Drive...\")\n",
        "    dataset_path = '/content/drive/MyDrive/IU-Xray'  # Updated path\n",
        "    train_img_paths, val_img_paths, test_img_paths, train_reports, val_reports, test_reports, clinical_data = load_indiana_cxr_data(dataset_path)\n",
        "\n",
        "    print(f\"Training samples: {len(train_img_paths)}\")\n",
        "    print(f\"Validation samples: {len(val_img_paths)}\")\n",
        "    print(f\"Test samples: {len(test_img_paths)}\")\n",
        "\n",
        "    # Create datasets with clinical context\n",
        "    train_dataset = EnhancedChestXRayDataset(\n",
        "        train_img_paths, train_reports, clinical_data, tokenizer,\n",
        "        transform=transform_train, max_length=CONFIG['max_report_length']\n",
        "    )\n",
        "\n",
        "    val_dataset = EnhancedChestXRayDataset(\n",
        "        val_img_paths, val_reports, clinical_data, tokenizer,\n",
        "        transform=transform_val, max_length=CONFIG['max_report_length']\n",
        "    )\n",
        "\n",
        "    test_dataset = EnhancedChestXRayDataset(\n",
        "        test_img_paths, test_reports, clinical_data, tokenizer,\n",
        "        transform=transform_val, max_length=CONFIG['max_report_length']\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing enhanced model...\")\n",
        "    model = EnhancedRadiologyReportGenerator(CONFIG).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "    total_steps = len(train_dataloader) * CONFIG['epochs']\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=CONFIG['learning_rate'],\n",
        "        steps_per_epoch=len(train_dataloader), epochs=CONFIG['epochs']\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 2\n",
        "    no_improve = 0\n",
        "\n",
        "    for epoch in range(CONFIG['epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{CONFIG['epochs']}\")\n",
        "        train_metrics = train_epoch_enhanced(\n",
        "            model, train_dataloader, optimizer, scheduler, device\n",
        "        )\n",
        "\n",
        "        print(f\"Train loss: {train_metrics['total_loss']:.4f}\")\n",
        "\n",
        "        eval_results = evaluate_enhanced(\n",
        "            model, val_dataloader, tokenizer, device\n",
        "        )\n",
        "\n",
        "        val_loss = eval_results['loss']\n",
        "        bleu_score = eval_results['bleu']\n",
        "        rouge_scores = eval_results['rouge']\n",
        "\n",
        "        print(f\"Validation loss: {val_loss:.4f}\")\n",
        "        print(f\"BLEU score: {bleu_score:.4f}\")\n",
        "        print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "        print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "        print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "        print(\"\\nSample predictions:\")\n",
        "        for i, sample in enumerate(eval_results['samples'][:2]):\n",
        "            print(f\"Example {i+1}:\")\n",
        "            if sample['clinical_context']:\n",
        "                print(f\"Clinical context: {sample['clinical_context']}\")\n",
        "            print(f\"Generated: {sample['generated']}\")\n",
        "            print(f\"Reference: {sample['reference']}\")\n",
        "            print()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improve = 0\n",
        "            print(\"Saving best model...\")\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    print(\"\\nEvaluating on test set...\")\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "    test_results = evaluate_enhanced(model, test_dataloader, tokenizer, device)\n",
        "\n",
        "    print(f\"Test BLEU score: {test_results['bleu']:.4f}\")\n",
        "    print(f\"Test ROUGE-L: {test_results['rouge']['rougeL']:.4f}\")\n",
        "\n",
        "    print(\"\\nAnalyzing medical accuracy...\")\n",
        "    med_accuracy = analyze_medical_accuracy(\n",
        "        [s['generated'] for s in test_results['samples']],\n",
        "        [s['reference'] for s in test_results['samples']]\n",
        "    )\n",
        "\n",
        "    print(f\"Medical term F1 score: {med_accuracy['avg_f1']:.4f}\")\n",
        "    print(f\"Negation accuracy: {med_accuracy['negation_accuracy']['overall_accuracy']:.4f}\")\n",
        "    print(\"Training complete!\")\n"
      ],
      "metadata": {
        "id": "hhjTFDmSXegR"
      },
      "id": "hhjTFDmSXegR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###########################################\n",
        "# 6. Dataset Details and Helper Functions\n",
        "###########################################"
      ],
      "metadata": {
        "id": "AIhWLDWLX--2"
      },
      "id": "AIhWLDWLX--2"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jWSHP33EXq3",
        "outputId": "d728ed5c-f104-4f2f-adc1-e71313bfd281"
      },
      "id": "4jWSHP33EXq3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `First Token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `First Token`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub[hf_xet]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t0qWbtMUZgK",
        "outputId": "5ad31851-b683-4e56-a024-e632b62ab2d5"
      },
      "id": "3t0qWbtMUZgK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_xet]) (4.13.1)\n",
            "Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet])\n",
            "  Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
            "Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf-xet\n",
            "Successfully installed hf-xet-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40bc9f1a",
      "metadata": {
        "id": "40bc9f1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64023294-6bc3-464a-b930-43ec1b2ab6ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IU X-Ray dataset from Google Drive...\n",
            "Training samples: 5245\n",
            "Validation samples: 1118\n",
            "Test samples: 1103\n",
            "Initializing enhanced model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-58-ae9452fd9ba3>:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # INITIALIZE SCALER HERE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/164 [00:00<?, ?it/s]<ipython-input-58-ae9452fd9ba3>:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # WRAP FORWARD PASS IN AUTOCAST\n",
            "Training:  40%|      | 65/164 [1:11:17<1:31:18, 55.34s/it, loss=11.5, report_loss=0, finding_loss=0]Process Process-20:\n",
            "Process Process-17:\n",
            "Process Process-19:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 317, in _bootstrap\n",
            "    util._exit_function()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 363, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 363, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 363, in _exit_function\n",
            "    _run_finalizers()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 199, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 199, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 199, in _finalize_join\n",
            "    thread.join()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1119, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "def get_dataset_info():\n",
        "    \"\"\"\n",
        "    Print information about available public chest X-ray datasets\n",
        "    \"\"\"\n",
        "    datasets = {\n",
        "        \"MIMIC-CXR\": {\n",
        "            \"description\": \"MIMIC Chest X-ray (MIMIC-CXR) Database is a large publicly available dataset of chest radiographs with free-text radiology reports.\",\n",
        "            \"size\": \"377,110 images from 227,835 radiographic studies\",\n",
        "            \"features\": \"Chest X-rays with associated radiology reports, patient metadata, and CheXpert labels\",\n",
        "            \"access\": \"PhysioNet Credentialed Access (requires training and data use agreement)\",\n",
        "            \"url\": \"https://physionet.org/content/mimic-cxr/2.0.0/\",\n",
        "            \"paper\": \"Johnson AEW, et al. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Scientific Data (2019).\"\n",
        "        },\n",
        "        \"Indiana CXR (Open-I)\": {\n",
        "            \"description\": \"Open-I Chest X-ray collection from Indiana University\",\n",
        "            \"size\": \"~8,000 chest X-rays with reports\",\n",
        "            \"features\": \"Images with findings, impression, and indications\",\n",
        "            \"access\": \"Public access, no restrictions\",\n",
        "            \"url\": \"https://openi.nlm.nih.gov/gridquery?it=xg&coll=cxr\",\n",
        "            \"paper\": \"Demner-Fushman D, et al. Preparing a collection of radiology examinations for distribution and retrieval. JAMIA (2016).\"\n",
        "        },\n",
        "        \"CheXpert\": {\n",
        "            \"description\": \"Large dataset of chest X-rays with labels for 14 common findings\",\n",
        "            \"size\": \"224,316 chest radiographs of 65,240 patients\",\n",
        "            \"features\": \"Labels for 14 observations, frontal and lateral views\",\n",
        "            \"access\": \"Public access, requires form submission\",\n",
        "            \"url\": \"https://stanfordmlgroup.github.io/competitions/chexpert/\",\n",
        "            \"paper\": \"Irvin J, et al. CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison. AAAI (2019).\"\n",
        "        },\n",
        "        \"PadChest\": {\n",
        "            \"description\": \"Large-scale chest X-ray dataset with multi-label annotations\",\n",
        "            \"size\": \"160,000 images from 67,000 patients\",\n",
        "            \"features\": \"Multiple views, radiographic reports and labels (extracted with NLP)\",\n",
        "            \"access\": \"Public access\",\n",
        "            \"url\": \"https://bimcv.cipf.es/bimcv-projects/padchest/\",\n",
        "            \"paper\": \"Bustos A, et al. PadChest: A large chest x-ray image dataset with multi-label annotated reports. Medical Image Analysis (2020).\"\n",
        "        },\n",
        "        \"RSNA Pneumonia Detection\": {\n",
        "            \"description\": \"Dataset for pneumonia detection from the RSNA Pneumonia Detection Challenge\",\n",
        "            \"size\": \"~30,000 chest X-rays\",\n",
        "            \"features\": \"Images with bounding box annotations for pneumonia\",\n",
        "            \"access\": \"Public access (Kaggle)\",\n",
        "            \"url\": \"https://www.kaggle.com/c/rsna-pneumonia-detection-challenge\",\n",
        "            \"paper\": \"RSNA Challenge (2018)\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for name, info in datasets.items():\n",
        "        print(f\"\\n{name}:\")\n",
        "        for key, value in info.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "    print(\"\\n\\nRecommended dataset combinations for this project:\")\n",
        "    print(\"1. MIMIC-CXR (primary) - provides both images and associated reports\")\n",
        "    print(\"2. MIMIC-CXR + CheXpert labels - for improved clinical accuracy validation\")\n",
        "    print(\"3. Indiana CXR (Open-I) - smaller but publicly accessible without restrictions\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}