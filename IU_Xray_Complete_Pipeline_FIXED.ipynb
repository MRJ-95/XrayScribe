{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2472b7",
   "metadata": {
    "id": "3b2472b7"
   },
   "source": [
    "# IU X-Ray Complete Pipeline Notebook\n",
    "\n",
    "This notebook includes:\n",
    "- Multi-label classification training of DenseNet-121 with data augmentation and weighted loss\n",
    "- Report generation fine-tuning of Flan-T5-small with LoRA\n",
    "- Inference & reporting producing a CSV of results with BLEU & ROUGE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2016427",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2016427",
    "outputId": "09ff5488-3b40-4dc5-8a69-e299da11f542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m842.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet torch torchvision transformers torchxrayvision scikit-learn rouge-score nltk datasets pydicom peft accelerate fuzzywuzzy python-Levenshtein matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3acef90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3acef90",
    "outputId": "2ec1ff5c-d924-4a3e-c3c8-1e9cab96a158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678a2299",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "678a2299",
    "outputId": "113f52ec-35b4-4d39-84c6-03786cb53574"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchxrayvision/utils.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torchxrayvision as xrv\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "CONFIG = {\n",
    "    'data_dir': '/content/drive/MyDrive/IU-Xray',\n",
    "    'image_dir': '/content/drive/MyDrive/IU-Xray/images/images_normalized',\n",
    "    'chexpert_labels_file': '/content/drive/MyDrive/IU-Xray/chexpert_labels.txt',\n",
    "    'batch_size_cls': 8,\n",
    "    'epochs_cls': 3,\n",
    "    'lr_cls': 1e-4,\n",
    "    'image_size': 224,\n",
    "    't5_model': 'google/flan-t5-small',\n",
    "    'lora_r': 8,\n",
    "    'lora_alpha': 16,\n",
    "    'chexpert_labels_file': '/content/drive/MyDrive/IU-Xray/chexpert_labels.txt',\n",
    "    'output_csv': '/content/iu_xray_full_results.csv'\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845d87bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "845d87bc",
    "outputId": "1d0b91be-2c5d-4438-e1d6-42cd8db88df9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val sizes: 1160 291\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data\n",
    "df = pd.read_csv(f\"{CONFIG['data_dir']}/merged_preprocessed.csv\")\n",
    "\n",
    "# Stratified split\n",
    "df['first_label'] = df['chexpert_labels'].apply(lambda x: x.split(';')[0] if isinstance(x, str) and x else 'None')\n",
    "# Filter out classes with only one sample in 'first_label'\n",
    "value_counts = df['first_label'].value_counts()\n",
    "df = df[df['first_label'].isin(value_counts[value_counts > 1].index)]\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, stratify=df['first_label'])\n",
    "print('Train/Val sizes:', len(df_train), len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3ffe8f",
   "metadata": {
    "id": "2a3ffe8f"
   },
   "outputs": [],
   "source": [
    "# Load CheXpert labels\n",
    "with open(CONFIG['chexpert_labels_file']) as f:\n",
    "    CHEXPERT_LABELS = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Manual mapping for non-matches\n",
    "MANUAL_MAP = {\n",
    "    'pulmonary fibrosis': 'Fibrosis',\n",
    "    'interstitial fibrosis': 'Fibrosis'\n",
    "}\n",
    "\n",
    "def map_mesh_to_chexpert(mesh_terms):\n",
    "    terms = [t.strip().lower() for t in mesh_terms.split(';') if t.strip()]\n",
    "    mapped = set()\n",
    "    for t in terms:\n",
    "        for lbl in CHEXPERT_LABELS:\n",
    "            if lbl.lower() in t or fuzz.partial_ratio(lbl.lower(), t) > 80:\n",
    "                mapped.add(lbl)\n",
    "        for k,v in MANUAL_MAP.items():\n",
    "            if k in t:\n",
    "                mapped.add(v)\n",
    "    return list(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec8d52b7",
   "metadata": {
    "id": "ec8d52b7"
   },
   "outputs": [],
   "source": [
    "# Data augmentation transforms\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(10),\n",
    "    T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    T.ToTensor(),\n",
    "    # The following line is changed:\n",
    "    lambda x: x.repeat(3, 1, 1), # Repeat grayscale channel 3 times\n",
    "    T.Normalize([0.485, 0.485, 0.485], [0.229, 0.229, 0.229]),\n",
    "    # lambda x: x.unsqueeze(0) # Removed unsqueeze as the model expects (C, H, W)\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
    "    T.ToTensor(),\n",
    "    # The following line is changed:\n",
    "    lambda x: x.repeat(3, 1, 1), # Repeat grayscale channel 3 times\n",
    "    T.Normalize([0.485, 0.485, 0.485], [0.229, 0.229, 0.229]),\n",
    "    # lambda x: x.unsqueeze(0) # Removed unsqueeze as the model expects (C, H, W)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a529986",
   "metadata": {
    "id": "3a529986"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, df, split_size, mode='cls'):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.split_size = split_size\n",
    "        self.mode = mode\n",
    "        if mode=='txt':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(CONFIG['t5_model'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      row = self.df.iloc[idx]\n",
    "      # Load normalized PNG image\n",
    "      img_path = os.path.join(CONFIG['image_dir'], row.filename)\n",
    "      img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "      if self.mode=='cls':\n",
    "        transform = train_transform if idx < self.split_size else val_transform\n",
    "        img = transform(img)\n",
    "        labels = row['chexpert_labels']  # Use pre-mapped labels\n",
    "        y = torch.tensor([1 if lbl in labels else 0 for lbl in CHEXPERT_LABELS], dtype=torch.float32)\n",
    "        return img, y\n",
    "      else:\n",
    "        img = val_transform(img)\n",
    "        with torch.no_grad():\n",
    "            logits = cls_model(img.unsqueeze(0).to(device))\n",
    "        probs = torch.sigmoid(logits)[0].cpu().numpy()\n",
    "        findings = [lbl for i,lbl in enumerate(CHEXPERT_LABELS) if probs[i]>0.5]\n",
    "        prompt = f\"History: {row.indication_clean}. Findings: {', '.join(findings)}\"\n",
    "        enc = self.tokenizer(prompt, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        tgt = self.tokenizer(row.impression_clean, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc.input_ids.squeeze(),\n",
    "            'attention_mask': enc.attention_mask.squeeze(),\n",
    "            'labels': tgt.input_ids.squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7d3b3be",
   "metadata": {
    "id": "c7d3b3be"
   },
   "outputs": [],
   "source": [
    "# Prepare loaders with weighted sampling\n",
    "train_ds = XRayDataset(df_train, len(df_train), mode='cls')\n",
    "val_ds   = XRayDataset(df_val, len(df_train), mode='cls')\n",
    "\n",
    "# Compute pos weights\n",
    "ys = torch.stack([y for _,y in train_ds])\n",
    "pos_counts = ys.sum(dim=0)\n",
    "total = ys.size(0)\n",
    "pos_weight = (total - pos_counts)/(pos_counts+1e-8)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "\n",
    "# Sampler\n",
    "weights = ys.sum(dim=1).numpy()\n",
    "sampler = WeightedRandomSampler(weights, num_samples=total, replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size_cls'], sampler=sampler)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CONFIG['batch_size_cls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91fa2cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e91fa2cc",
    "outputId": "fabe6e07-7195-465e-adbb-a01bbc3e5db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Loss: 1.3595\n",
      "Epoch 2/3 Loss: 1.2815\n",
      "Epoch 3/3 Loss: 1.3274\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "               Atelectasis       0.19      1.00      0.32        56\n",
      "              Cardiomegaly       0.24      0.95      0.38        62\n",
      "             Consolidation       0.01      1.00      0.02         3\n",
      "                     Edema       0.02      1.00      0.05         7\n",
      "                  Effusion       0.08      1.00      0.14        22\n",
      "Enlarged Cardiomediastinum       0.00      0.00      0.00         0\n",
      "                  Fracture       0.09      0.43      0.15        28\n",
      "               Lung Lesion       0.24      0.66      0.35        53\n",
      "              Lung Opacity       0.36      1.00      0.53       105\n",
      "                No Finding       0.00      0.00      0.00        17\n",
      "          Pleural Effusion       0.08      1.00      0.14        22\n",
      "             Pleural Other       0.00      0.00      0.00         0\n",
      "                 Pneumonia       0.02      1.00      0.05         7\n",
      "              Pneumothorax       0.02      1.00      0.04         6\n",
      "           Support Devices       0.00      0.00      0.00         0\n",
      "\n",
      "                 micro avg       0.12      0.86      0.21       388\n",
      "                 macro avg       0.09      0.67      0.14       388\n",
      "              weighted avg       0.21      0.86      0.33       388\n",
      "               samples avg       0.12      0.75      0.20       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cls_model = xrv.models.DenseNet(weights='densenet121-res224-chex').to(device)\n",
    "# Access classifier's in_features directly from cls_model.features\n",
    "cls_model.classifier = torch.nn.Linear(cls_model.features.norm5.num_features, len(CHEXPERT_LABELS)).to(device)  # Replace 'model.classifier' with 'features.norm5'\n",
    "# The following line is changed:\n",
    "cls_model.features.conv0 = torch.nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).to(device) # Modify the first convolutional layer to accept 3 channels\n",
    "# The following line is added to fix the error\n",
    "cls_model.op_threshs = None # Reset op_threshs to avoid dimension mismatch\n",
    "\n",
    "optimizer = torch.optim.AdamW(cls_model.parameters(), lr=CONFIG['lr_cls'])\n",
    "\n",
    "\n",
    "for epoch in range(CONFIG['epochs_cls']):\n",
    "    cls_model.train()\n",
    "    total_loss = 0\n",
    "    for imgs,labels in train_loader:\n",
    "        imgs,labels = imgs.to(device), labels.to(device)\n",
    "        preds = cls_model(imgs)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{CONFIG['epochs_cls']} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Validation\n",
    "cls_model.eval()\n",
    "y_t, y_p = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs,labels in val_loader:\n",
    "        out = torch.sigmoid(cls_model(imgs.to(device))).cpu().numpy()\n",
    "        y_p.extend((out>0.5).astype(int)); y_t.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(y_t, y_p, target_names=CHEXPERT_LABELS, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46d5631a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46d5631a",
    "outputId": "9215bed9-c432-4a85-c6f7-7a3c0b4c55b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved DenseNet model.\n"
     ]
    }
   ],
   "source": [
    "# Save trained classification model\n",
    "torch.save(cls_model.state_dict(), '/content/densenet_finetuned.pt')\n",
    "print(\"Saved DenseNet model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "735aef74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "735aef74",
    "outputId": "fe473473-d247-4b40-990f-b66df28abee9"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at '/content/drive/MyDrive/lora_models/t5_lora_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    261\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/lora_models/t5_lora_model'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4b3a3f84fcdb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbase_t5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't5_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Update the path to the LoRA model if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m lf_model = PeftModel.from_pretrained(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbase_t5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/lora_models/t5_lora_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 439\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 )\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/content/drive/MyDrive/lora_models/t5_lora_model'"
     ]
    }
   ],
   "source": [
    "# Inference & Reporting\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load text model\n",
    "base_t5 = AutoModelForSeq2SeqLM.from_pretrained(CONFIG['t5_model'])\n",
    "# Update the path to the LoRA model if necessary\n",
    "lf_model = PeftModel.from_pretrained(\n",
    "    base_t5,\n",
    "    model_id=\"/content/drive/MyDrive/lora_models/t5_lora_model\",\n",
    "    is_local=True\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['t5_model'])\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "bleu_fn = SmoothingFunction().method1\n",
    "\n",
    "results = []\n",
    "for _,row in df.iterrows():\n",
    "    dcm_path = os.path.join(CONFIG['image_dir'],row.filename.replace('.png','.dcm'))\n",
    "    ds = pydicom.dcmread(dcm_path)\n",
    "    img = (torch.tensor(ds.pixel_array.astype(np.float32)).unsqueeze(0)-ds.pixel_array.mean())/ds.pixel_array.std()\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = cls_model(img)\n",
    "    probs = torch.sigmoid(logits)[0].cpu().numpy()\n",
    "    preds = [lbl for i,lbl in enumerate(CHEXPERT_LABELS) if probs[i]>0.5]\n",
    "    prompt0 = f\"Findings: {', '.join(preds)}. Generate a radiology report.\"\n",
    "    rpt0 = lf_model.generate(**tokenizer(prompt0, return_tensors='pt', truncation=True).to(device), max_length=256)\n",
    "    text0 = tokenizer.decode(rpt0[0], skip_special_tokens=True)\n",
    "    prompt1 = f\"History: {row.indication_clean}. Findings: {', '.join(preds)}. Generate a refined radiology report.\"\n",
    "    rpt1 = lf_model.generate(**tokenizer(prompt1, return_tensors='pt', truncation=True).to(device), max_length=256)\n",
    "    text1 = tokenizer.decode(rpt1[0], skip_special_tokens=True)\n",
    "    r1 = rouge.score(row.impression_clean, text0)['rouge1'].fmeasure\n",
    "    b1 = sentence_bleu([row.impression_clean.split()], text0.split(), smoothing_function=bleu_fn)\n",
    "    r2 = rouge.score(row.impression_clean, text1)['rouge1'].fmeasure\n",
    "    b2 = sentence_bleu([row.impression_clean.split()], text1.split(), smoothing_function=bleu_fn)\n",
    "    results.append({\n",
    "        'uid': row.uid,\n",
    "        'predicted_labels': ';'.join(preds),\n",
    "        'report_initial': text0,\n",
    "        'report_refined': text1,\n",
    "        'rouge1_init': r1,\n",
    "        'rouge1_ref': r2,\n",
    "        'bleu_init': b1,\n",
    "        'bleu_ref': b2\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results).to_csv(CONFIG['output_csv'], index=False)\n",
    "print(f\"Saved results to {CONFIG['output_csv']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51e0f1",
   "metadata": {},
   "source": [
    "## 🔥 Fine-tune Flan-T5-small with LoRA (NEWLY ADDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Reload Flan-T5 base model\n",
    "base_t5 = AutoModelForSeq2SeqLM.from_pretrained(CONFIG['t5_model'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['t5_model'])\n",
    "\n",
    "# Apply LoRA\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=16)\n",
    "lf_model = get_peft_model(base_t5, peft_config).to(device)\n",
    "\n",
    "# Prepare dataset for text fine-tuning\n",
    "txt_ds = XRayDataset(df, len(df), mode='txt')\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/content/output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"/content/logs\",\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=lf_model,\n",
    "    args=training_args,\n",
    "    train_dataset=txt_ds,\n",
    "    eval_dataset=txt_ds\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save LoRA model\n",
    "lf_model.save_pretrained(CONFIG['lora_save_dir'])\n",
    "print(f\"✅ LoRA model saved at {CONFIG['lora_save_dir']}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}